# -*- coding: utf-8 -*-
"""ARC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uuFjx6oMX8xx_wTmFhuqbHJPqlKxziY7
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install colorama
!pip install utils
!pip install xLSTM

# Commented out IPython magic to ensure Python compatibility.
import time

import os, gc
import sys, pdb
import copy, time
import json, random

import itertools
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from pathlib import Path

import matplotlib
from matplotlib import colors
import matplotlib.pyplot as plt
#from utils import plot_pic

from colorama import Style, Fore
from tqdm import tqdm

# %matplotlib inline

import numpy as np
import json
import os
from tqdm import tqdm
import torch
from torch.utils.data import DataLoader
from xlstm.xlstm_lm_model import xLSTMLMModel, xLSTMLMModelConfig
from torch import nn, optim
from torch.utils.data import Dataset
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

from torchvision import transforms
!nvidia-smi
!CUDA_HOME='/usr/local/cuda'

augmentation_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
])

!ls /content/drive/MyDrive/2024-2/ARC/ARCPrice2024/arc-prize-2024-colab/

cmap = colors.ListedColormap(
    ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',
     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])


norm = colors.Normalize(vmin=0, vmax=9)
color_list = ["black", "blue", "red", "green", "yellow", "gray", "magenta", "orange", "sky", "brown"]

values = np.arange(10)

fig, ax = plt.subplots(figsize=(8, 1))
ax.imshow([values], cmap=cmap, norm=norm)
ax.set_xticks(np.arange(len(color_list)))
ax.set_xticklabels(color_list)
ax.set_yticks([])
plt.show()

# Define the maximum grid size based on the dataset analysis
# Define the maximum grid size based on the dataset analysis
MAX_GRID_SIZE = 30  # Adjust as needed

def pad_grid(grid, size):
    h, w = grid.shape
    padded_grid = np.zeros((size, size), dtype=grid.dtype)
    padded_grid[:h, :w] = grid
    return padded_grid

def grid_to_sequence(grid):
    # Flatten the grid to create a sequence
    return grid.flatten()

def load_arc_data(challenges_path):
    with open(challenges_path, 'r') as f:
        challenges_data = json.load(f)

    inputs, outputs = [], []

    for task_id, task_data in tqdm(challenges_data.items(), desc="Loading tasks"):
        # Process training examples only
        for example in task_data["train"]:
            input_grid = np.array(example["input"])
            output_grid = np.array(example["output"])

            # Standardize grid sizes
            input_grid_padded = pad_grid(input_grid, size=MAX_GRID_SIZE)
            output_grid_padded = pad_grid(output_grid, size=MAX_GRID_SIZE)

            # Normalize grid values
            input_grid_normalized = input_grid_padded / 9.0
            output_grid_normalized = output_grid_padded / 9.0

            # Convert grids to sequences
            input_sequence = grid_to_sequence(input_grid_normalized)
            output_sequence = grid_to_sequence(output_grid_normalized)

            # Append sequences to lists
            inputs.append(input_sequence)
            outputs.append(output_sequence)

    return inputs, outputs

# Paths to your data files
train1_path = '/content/drive/MyDrive/2024-2/ARC/ARCPrice2024/arc-prize-2024-colab/arc-agi_training_challenges.json'

# Load and preprocess data
inputs, outputs = load_arc_data(train1_path)

# Verify data loading
print(f"Number of samples loaded: {len(inputs)}")
if len(inputs) == 0:
    print("No data was loaded. Please check your data paths and preprocessing steps.")
else:
    print(f"Sample input sequence length: {len(inputs[0])}")
    print(f"Sample output sequence length: {len(outputs[0])}")

class xLSTMModelClassification(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(xLSTMModelClassification, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Define LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # Define a fully connected layer to map hidden states to class scores
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        # Initialize hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))

        # Reshape and decode hidden states of all time steps
        out = out.reshape(-1, self.hidden_size)
        out = self.fc(out)  # [batch_size*seq_length, num_classes]

        # Reshape back to [batch_size, seq_length, num_classes]
        out = out.reshape(x.size(0), -1, num_classes)
        return out

class ARCDataset(Dataset):
    def __init__(self, inputs, outputs):
        self.inputs = [torch.tensor(seq, dtype=torch.float32) for seq in inputs]  # [seq_length]
        self.outputs = [torch.tensor(seq, dtype=torch.float32) for seq in outputs]  # [seq_length]

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx], self.outputs[idx]

# Create DataLoader without custom collate_fn
train_loader = DataLoader(dataset, batch_size=64, shuffle=True)

# Training function with input reshaping
def train_model(model, dataloader, optimizer, criterion, epochs=10):
    device = next(model.parameters()).device
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for inputs, targets in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            # Reshape inputs
            inputs = inputs.unsqueeze(-1).to(device)  # [batch_size, seq_length, 1]
            targets = targets.to(device)              # [batch_size, seq_length]

            optimizer.zero_grad()
            outputs = model(inputs)                   # [batch_size, seq_length]
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

# Initialize and train the model
model = xLSTMModel(input_size=1, hidden_size=128, num_layers=2, output_size=1)
model = model.to('cuda' if torch.cuda.is_available() else 'cpu')
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

train_model(model, train_loader, optimizer, criterion, epochs=10)
def infer(model, input_sequence):
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)
        output_sequence = model(input_tensor)
        output_sequence = output_sequence.squeeze(0).cpu().numpy()
    return output_sequence
def collate_fn(batch):
    inputs, targets = zip(*batch)
    inputs = torch.stack(inputs)   # Each input is [seq_length, 1], resulting in [batch_size, seq_length, 1]
    targets = torch.stack(targets) # Each target is [seq_length], resulting in [batch_size, seq_length]
    return inputs, targets

# Create dataset and dataloader
dataset = ARCDataset(inputs, outputs)
train_loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)

"""###inputs shape before unsqueeze: torch.Size([64, 900])
inputs shape after unsqueeze: torch.Size([64, 900, 1])
targets shape: torch.Size([64, 900])###
"""

# Model parameters
input_size = 1          # Each element in the sequence is a single float value
hidden_size = 128       # Number of features in hidden state
num_layers = 2          # Number of stacked LSTM layers
output_size = 900       # Output sequence length (same as input)

# Initialize the model
model = xLSTMModel(input_size, hidden_size, num_layers, output_size)
model = model.to('cuda' if torch.cuda.is_available() else 'cpu')

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
train_model(model, train_loader, optimizer, criterion, epochs=32)

def visualize_grids(input_grid, predicted_grid, target_grid=None):
    fig, axs = plt.subplots(1, 3 if target_grid is not None else 2, figsize=(15, 5))

    axs[0].imshow(input_grid, cmap=cmap, norm=norm)
    axs[0].set_title('Input Grid')

    axs[1].imshow(predicted_grid, cmap=cmap, norm=norm)
    axs[1].set_title('Predicted Output Grid')

    if target_grid is not None:
        axs[2].imshow(target_grid, cmap=cmap, norm=norm)
        axs[2].set_title('Target Output Grid')

    plt.show()

# Select a sample from the dataset
sample_idx = 0  # Change index to test different samples
input_seq, target_seq = dataset[sample_idx]
input_seq = input_seq.squeeze(-1).numpy()    # Shape: [seq_length]
target_seq = target_seq.numpy()

# Run inference
predicted_seq = infer(model, input_seq)

# Reshape sequences back to grids
input_grid = input_seq.reshape(MAX_GRID_SIZE, MAX_GRID_SIZE) * 9.0
predicted_grid = predicted_seq.reshape(MAX_GRID_SIZE, MAX_GRID_SIZE) * 9.0
target_grid = target_seq.reshape(MAX_GRID_SIZE, MAX_GRID_SIZE) * 9.0

# Round the values to the nearest integer and clip to valid color range
input_grid = np.clip(np.round(input_grid), 0, 9).astype(int)
predicted_grid = np.clip(np.round(predicted_grid), 0, 9).astype(int)
target_grid = np.clip(np.round(target_grid), 0, 9).astype(int)

# Visualize the grids
visualize_grids(input_grid, predicted_grid, target_grid)



class xLSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(xLSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Define LSTM layer
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # Define a fully connected layer to map hidden states to output
        self.fc = nn.Linear(hidden_size, 1)  # Output size is 1 for each time step

    def forward(self, x):
        # x shape: [batch_size, seq_length, input_size]
        batch_size = x.size(0)
        seq_length = x.size(1)

        # Initialize hidden and cell states
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))  # out shape: [batch_size, seq_length, hidden_size]

        # Pass through the fully connected layer
        out = self.fc(out)  # out shape: [batch_size, seq_length, 1]

        # Remove the last dimension
        out = out.squeeze(-1)  # out shape: [batch_size, seq_length]

        return out

# Model parameters
input_size = 1  # Sinc

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

def train_model(model, dataloader, optimizer, criterion, epochs=10):
    model.train()
    device = next(model.parameters()).device

    for epoch in range(epochs):
        total_loss = 0
        for inputs, targets in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            inputs = inputs.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)

            loss = criterion(outputs, targets)
            loss.backward()

            # Gradient clipping
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)

            optimizer.step()
            total_loss += loss.item()

        average_loss = total_loss / len(dataloader)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {average_loss:.4f}")

# Train the model
train_model(model, train_loader, optimizer, criterion, epochs=20)

def infer(model, input_grid):
    model.eval()
    device = next(model.parameters()).device

    # Preprocess input grid
    input_grid_padded = pad_grid(input_grid, size=MAX_GRID_SIZE)
    input_grid_normalized = input_grid_padded / 9.0
    input_sequence = grid_to_sequence(input_grid_normalized)
    input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).to(device)

    with torch.no_grad():
        output_sequence = model(input_tensor)
        output_sequence = output_sequence.squeeze(0).cpu().numpy()

    # Convert sequence back to grid
    output_grid = sequence_to_grid(output_sequence, size=MAX_GRID_SIZE)

    # Denormalize grid values
    output_grid_denormalized = (output_grid * 9.0).round().astype(int)
    return output_grid_denormalized

def sequence_to_grid(sequence, size):
    return sequence.reshape(size, size)

# Example inference
test_input_grid = np.array([[...], [...], ...])  # Replace with actual input grid
predicted_output_grid = infer(model, test_input_grid)

# Visualize or save the predicted output grid

# Commented out IPython magic to ensure Python compatibility.
# %cd $visual_path
!ls

def visualize_grids(input_grid, predicted_grid, target_grid=None):
    fig, axs = plt.subplots(1, 3 if target_grid is not None else 2, figsize=(15, 5))

    axs[0].imshow(input_grid, cmap=cmap, norm=norm)
    axs[0].set_title('Input Grid')

    axs[1].imshow(predicted_grid, cmap=cmap, norm=norm)
    axs[1].set_title('Predicted Output Grid')

    if target_grid is not None:
        axs[2].imshow(target_grid, cmap=cmap, norm=norm)
        axs[2].set_title('Target Output Grid')

    plt.show()

# Visualize the result
visualize_grids(test_input_grid, predicted_output_grid)



